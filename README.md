# ğŸ§  Empathetic Best-Friend Chatbot  
**Base vs SFT Evaluation using EQ-Bench-style Metrics**

## ğŸ“Œ Overview
This project fine-tunes an open-weight large language model to behave as an **empathetic, emotionally supportive best-friend chatbot**, and evaluates measurable improvements over a base model using an **EQ-Bench-style empathy benchmark**.

The work follows a **low-compute setup** using **PEFT / QLoRA** and is designed to be **reproducible, transparent, and resource-aware**, as required by the assignment.

---

## ğŸ¯ Objective
- Establish a **baseline empathy score** for a base model  
- Apply **Supervised Fine-Tuning (SFT)** focused on empathetic response quality  
- Measure improvement using the **same evaluation pipeline**  
- Analyze training behavior under **limited GPU resources**  
- Demonstrate **early stopping and overfitting awareness**

---

## ğŸ–¥ï¸ Compute Environment & Constraints
- **Platform:** Google Colab  
- **GPU:** NVIDIA T4 (~15 GB VRAM)  
- **Precision:** 4-bit quantization (QLoRA)  
- **Training Strategy:** Parameter-Efficient Fine-Tuning (LoRA)

**Note on Compute Limits:**  
Due to T4 GPU constraints, full-epoch SFT on large datasets is not feasible. Training is intentionally limited to short, controlled steps, prioritizing learning signal quality over brute-force optimization. This reflects real-world low-resource fine-tuning scenarios.

---

## ğŸ¤– Model Details
- **Base Model:** `TinyLlama/TinyLlama-1.1B-Chat-v1.0`  
- **Fine-Tuning Method:** QLoRA (4-bit)  
- **Trainable Parameters:** ~0.2â€“0.4% of total parameters  
- **Frozen Base Weights:** Yes  

This setup satisfies the **Low-Compute Track** requirements.

---

## ğŸ“Š Datasets Used
A curated **multi-task empathy mixture** was prepared and reused across all experiments:

| Dataset | Purpose |
|------|--------|
| EmpatheticDialogues | Empathetic conversational grounding |
| GoEmotions | Emotion classification supervision |
| ESConv | Emotional support strategy learning |

- Data saved locally as JSONL (`train/dev`)  
- Same dev set used for **Base, SFT, and Continued SFT**  
- Ensures fair and controlled comparison

---

## ğŸ§ª Evaluation Protocol (EQ-Bench Style)

Empathy is scored using a **5-point rubric**:

1. Not empathetic  
2. Weak empathy  
3. Neutral / generic  
4. Clearly empathetic  
5. Highly empathetic and supportive  

- 30 fixed prompts sampled from the dev set  
- Identical prompts, decoding strategy, and judge logic for all models  
- Guarantees scientific validity

---

## ğŸ“ˆ Results Summary

### ğŸ”¹ Base Model
```
EQ-Bench (Base): 3.767
```

### ğŸ”¹ SFT (~100 steps)
```
EQ-Bench (SFT-100): 3.800
Î” Improvement: +0.033
```

### ğŸ”¹ Continued SFT (+50 steps)
```
EQ-Bench (SFT-150): 3.833
```

---

## ğŸ§  Interpretation of Results
- Initial SFT yields **measurable empathy improvement**
- Additional training introduces **performance regression**
- Indicates **early overfitting**, common in:
  - Small models  
  - Limited data regimes  
  - Short-context empathy tasks  

**Key takeaway:** More training does not always improve performance â€” early stopping is critical.

---

## ğŸ” Optimization Focus
Rather than increasing training length, emphasis was placed on:

- **Behavioral Alignment:**  
  Strong system prompts emphasizing emotional validation, warmth, and non-judgmental tone  

- **Parameter Efficiency:**  
  LoRA adapters targeting stylistic adaptation instead of factual memorization  

- **Learning Rate Control:**  
  Reduced LR during continuation to prevent instability  

- **Evaluation Fairness:**  
  Same evaluation pipeline across all runs (no cherry-picking)

---

## ğŸš¦ Why Training Was Limited
- Focus on demonstrating correct methodology, not maximal performance  
- T4 GPU limits prohibit long SFT runs  
- Assignment emphasizes:
  - Evaluation rigor  
  - Engineering hygiene  
  - Reproducibility  

---

## ğŸ“ Repository Structure
```
â”œâ”€â”€ BASE_VS_SFT.ipynb
â”œâ”€â”€ train_multitask.jsonl
â”œâ”€â”€ dev_multitask.jsonl
â”œâ”€â”€ sft_adapter/
â””â”€â”€ README.md
```

---

## ğŸ§¾ Reproducibility
- All datasets stored locally  
- LoRA adapters saved and reloadable  
- Evaluation prompts fixed  
- Notebook resumable without retraining base model  

---

## ğŸ Final Notes
This project demonstrates:
- Correct PEFT usage under low compute  
- Honest reporting of results  
- Understanding of overfitting and evaluation variance  
- Alignment with EQ-Bench-style empathy evaluation  

Designed to be **educational, reproducible, and assignment-compliant**.
