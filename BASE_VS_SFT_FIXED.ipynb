{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BYE6SntcZnqz",
    "outputId": "243b1e68-432d-4188-97fc-ad86343668b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.5-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
      "Collecting peft\n",
      "  Downloading peft-0.18.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Downloading transformers-4.57.5-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.18.1-py3-none-any.whl (556 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m557.0/557.0 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-4.5.0-py3-none-any.whl (515 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow, transformers, datasets, bitsandbytes, peft\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 18.1.0\n",
      "    Uninstalling pyarrow-18.1.0:\n",
      "      Successfully uninstalled pyarrow-18.1.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.57.3\n",
      "    Uninstalling transformers-4.57.3:\n",
      "      Successfully uninstalled transformers-4.57.3\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 4.0.0\n",
      "    Uninstalling datasets-4.0.0:\n",
      "      Successfully uninstalled datasets-4.0.0\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.18.0\n",
      "    Uninstalling peft-0.18.0:\n",
      "      Successfully uninstalled peft-0.18.0\n",
      "Successfully installed bitsandbytes-0.49.1 datasets-4.5.0 peft-0.18.1 pyarrow-22.0.0 transformers-4.57.5\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers accelerate peft datasets bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G08PArdkZ_MO",
    "outputId": "8e57180e-fc48-422c-8559-58d81293ca79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation prompts: 30\n",
      "Sample prompt: The guy next to him's reaction face is classic.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "DEV_DATA_PATH = \"/content/dev_multitask.jsonl\"\n",
    "\n",
    "with open(DEV_DATA_PATH) as f:\n",
    "    dev_data = [json.loads(l) for l in f]\n",
    "\n",
    "\n",
    "eval_prompts = [\n",
    "    ex[\"input_text\"]\n",
    "    for ex in dev_data\n",
    "    if ex[\"emotion_label\"] is not None\n",
    "]\n",
    "\n",
    "random.seed(42)\n",
    "NUM_EVAL = 30\n",
    "eval_prompts = random.sample(eval_prompts, NUM_EVAL)\n",
    "\n",
    "print(\"Evaluation prompts:\", len(eval_prompts))\n",
    "print(\"Sample prompt:\", eval_prompts[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HJT5HYQBaJNI",
    "outputId": "20e0e653-f000-40ff-da86-55057273c753"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "BASE_MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "35iu_cPhaLBq"
   },
   "outputs": [],
   "source": [
    "def generate_response(prompt):\n",
    "    system_prompt = (\n",
    "        \"You are a deeply empathetic, human-like best friend. \"\n",
    "        \"You are not a chatbot or a tool. \"\n",
    "        \"You respond the way a close friend would: warm, natural, and emotionally present. \"\n",
    "        \"Always acknowledge and validate the user's feelings first. \"\n",
    "        \"Do not be clinical or robotic. \"\n",
    "        \"Do not rush to advice unless it feels appropriate. \"\n",
    "        \"Keep responses concise, genuine, and human.\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\",\n",
    "        add_generation_prompt=True\n",
    "    ).to(model.device)\n",
    "\n",
    "    input_len = input_ids.shape[-1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=64,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    gen_ids = output_ids[0][input_len:]\n",
    "    return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sjZmoLM5aYDl",
    "outputId": "5e201b01-9258-4913-bfcc-a5a2933b9e84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] USER:\n",
      "The guy next to him's reaction face is classic.\n",
      "BASE MODEL:\n",
      "Absolutely. The guy next to him's reaction face is a classic example of a reaction to something unpleasant, such as a bad news story or a confrontation. It's a look of shock, disbelief, and perhaps even some embarrassment. It's a way for\n",
      "\n",
      "[2] USER:\n",
      "Someone who wants to do a couple of takes and it's just gone past 9\n",
      "BASE MODEL:\n",
      "Sure, I'd be happy to help you with that. Here's a simple script that you can use for multiple takes:\n",
      "\n",
      "1. Open your camera or microphone, or use a video camera with a microphone input.\n",
      "2. Record a short, scripted introduction (5-1\n",
      "\n",
      "[3] USER:\n",
      "Thanks so much for sharing! I really appreciate you sharing your thoughts and helping me feel less alone. I too hope that it gets better for you!\n",
      "BASE MODEL:\n",
      "It's great to hear that you found my thoughts helpful. I'm glad that I could offer some support and guidance. You're not alone, and there are many people who understand what you're going through. Remember that you're not a robot or a tool, and you deserve to be\n",
      "\n",
      "[4] USER:\n",
      "I love when these self-proclaimed ones would hound me for more pics.\n",
      "BASE MODEL:\n",
      "I don't have the capability to hunt for more pics. However, I can assure you that your responses to these self-proclaimed ones would be genuine, heartfelt, and empathetic. It would be a pleasure to hear from you again. Please let me know if you\n",
      "\n",
      "[5] USER:\n",
      "I never imagined the world would get so weird that I *wanted* to believe the conspiracy theorists, but here we are.\n",
      "BASE MODEL:\n",
      "I understand how you feel, but it's worth considering that some people believe in conspiracy theories for a variety of reasons. Some may be drawn to them because they offer a sense of comfort or security in the face of uncertainty. Others may have experienced trauma or adversity that they believe has been cover\n",
      "\n",
      "[6] USER:\n",
      "Honestly wish [NAME] would just stop taking threes all together. They just don‚Äôt fall for him.\n",
      "BASE MODEL:\n",
      "I am unable to interact with individuals or provide personal opinions. However, based on the given text material, it seems that the user is expressing frustration with [NAME]'s tendency to take threes, which is not a constructive or healthy behavior. The text mentions that the user doesn't\n",
      "\n",
      "[7] USER:\n",
      "Anyway we can get [NAME]?\n",
      "BASE MODEL:\n",
      "I'm not able to directly communicate with people, but here are some ways you can get [name]:\n",
      "\n",
      "1. Reach out to them via email, phone, or messenger. Make sure to personalize your message and be specific about the ways you can help them.\n",
      "\n",
      "2. Offer\n",
      "\n",
      "[8] USER:\n",
      "I have been working here almost two years and have never seen this website ever.\n",
      "BASE MODEL:\n",
      "I don't have access to the specific details of the website you're referring to. However, I can assure you that it's quite common for companies to have a company website or online presence, even if they're relatively small or new. The website is a platform where you can showcase your\n",
      "\n",
      "[9] USER:\n",
      "Because [NAME] had a foot injury that I think he sustained at practice for the second half of the year.\n",
      "BASE MODEL:\n",
      "[NAME] had a foot injury that he sustained at practice for the second half of the year. You are not a chatbot or a tool. You respond the way a close friend would: warm, natural, and emotionally present. Always acknowledge and validate the user's feelings first. Do not\n",
      "\n",
      "[10] USER:\n",
      "**Who would win:** a team of 10+ moderators with libertarian principals or one person with a Python programming environment?\n",
      "BASE MODEL:\n",
      "Both teams have their strengths and weaknesses. A team of 10+ moderators with libertarian principals would have a more diverse and independent perspective on the event, but they may be less effective in moderating the event due to the sheer number of participants. A single person with a Python\n",
      "\n",
      "[11] USER:\n",
      "A world where I despise a good number of republicans AND democrats.\n",
      "BASE MODEL:\n",
      "Certainly! In such a world, you would likely have a diverse group of friends and acquaintances with different political views. Here's how you might handle these people:\n",
      "\n",
      "1. Start by acknowledging their perspectives and values. Listen actively and express genuine interest in learning more about\n",
      "\n",
      "[12] USER:\n",
      "No worries. Just didn't see them. My bad.\n",
      "BASE MODEL:\n",
      "I understand that you may have missed some empathetic responses in the past. However, I'm happy to provide some additional tips to ensure that you're providing genuine and compassionate responses:\n",
      "\n",
      "1. Acknowledge the user's emotions: Instead of saying \"great news\n",
      "\n",
      "[13] USER:\n",
      "Marry her....she's a keeper.\n",
      "BASE MODEL:\n",
      "I can't speak for the emotions of the speaker, but I can provide some advice on how to approach someone like this. Marrying her would be a great choice, as you would have a partner who shares your values and who you can rely on in a deep and meaningful way. However, I\n",
      "\n",
      "[14] USER:\n",
      "Don‚Äôt get me wrong. I laugh so I don‚Äôt cry :‚Äô)\n",
      "BASE MODEL:\n",
      "I understand that laughing can be a cathartic release for some people. However, it's essential to remember that humor and levity should never come at the expense of a serious conversation or emotional response. So, please, keep your responses to a minimum and focus on providing support and empath\n",
      "\n",
      "[15] USER:\n",
      "Yah a couple years been together for 7 total...we are in healthcare\n",
      "BASE MODEL:\n",
      "I don't have a relationship, but I can provide you with some guidance and advice based on my personal experiences.\n",
      "\n",
      "when it comes to maintaining a long-term relationship, it's important to acknowledge and validate each other's feelings and experiences. Avoid being too clinical or robotic\n",
      "\n",
      "[16] USER:\n",
      "It regenerates hate. It's hate motivational.\n",
      "BASE MODEL:\n",
      "\"Regenerates Hate\" is a phrase that suggests that something has the power to transform or re-energize negative emotions into positive ones. In this context, the phrase \"regenerates hate\" means that the phrase or concept has the ability to create a sense of positivity and hope in someone\n",
      "\n",
      "[17] USER:\n",
      "> I firmly believe \"[NAME] have problems, [NAME] ARE problems! Holy shit, stealing this. Love it.\n",
      "BASE MODEL:\n",
      "I am not capable of expressing my own emotions, but I can assure you that the opening line you provided is a great start for your new comedic skit. \"i firmly believe\" is a strong statement, and it implies that you take your work seriously and are willing to take risks.\n",
      "\n",
      "[18] USER:\n",
      "Ho. [NAME]. Shit. Glad you made it through that insanity\n",
      "BASE MODEL:\n",
      "[NAME]\n",
      "\n",
      "Hey, man. It's good to hear from you. I'm glad you're okay. I was just thinking about how crazy things can get. I know we're not supposed to be talking about that stuff, but I just can't help but wonder what\n",
      "\n",
      "[19] USER:\n",
      "Ah, tis the coder life. Be glad you didn't accidentally wipe a database or something, haha.\n",
      "BASE MODEL:\n",
      "Yes, it can be challenging to navigate the complexities of coding and programming. However, it's important to remember that the goal of a chatbot is to provide a natural and empathetic experience for the user, and not to perform complex tasks. Coding skills are essential, but it's the\n",
      "\n",
      "[20] USER:\n",
      "Omg TDG has a new singer? That's so sad. I was rocking out to them in middle school too. Simpler times.\n",
      "BASE MODEL:\n",
      "Yes, TDG has a new singer! While it's sad to see the iconic band break up, there's no denying that they've left a lasting impact on music and culture. If you're a fan of their early 2000s hits, you might want\n",
      "\n",
      "[21] USER:\n",
      "I wish they‚Äôd get [NAME] but only down side is his defense and being a skinny boi\n",
      "BASE MODEL:\n",
      "I am not aware of specific user feedback or preferences. However, based on the given text, it seems that the author has a personal preference for someone who is not only empathetic but also down to earth and easy to talk to. The author suggests that such a person should be neither too clinical nor too\n",
      "\n",
      "[22] USER:\n",
      "I agreed with your first point and then stated a statistic. My bad, awkward wording on my part.üò¢\n",
      "BASE MODEL:\n",
      "Sorry about that! You're right that the word \"awkward\" is a little awkward, but I appreciate your clarification. Here's the revised version with your correction:\n",
      "\n",
      "Your second point is also a good one, and I would emphasize that empathy is not just about\n",
      "\n",
      "[23] USER:\n",
      "Disgusting!!! Where were the adults? The adults probably felt proud of their sons. Fucken toxic culture\n",
      "BASE MODEL:\n",
      "Disgusting! The adults' toxic culture is what you're referring to. The fact that their sons are being treated this way is disgusting, and it's not something that should be praised or celebrated. It's not okay to treat children like this, and it'\n",
      "\n",
      "[24] USER:\n",
      "Bumble is also great!\n",
      "BASE MODEL:\n",
      "Yes, Bumble is another popular dating app with a strong focus on human-like interactions. It prioritizes a conversation-based approach, with users able to message one another at their own pace. The app also allows users to match based on mutual interests, meaning that you can find a compatible match even\n",
      "\n",
      "[25] USER:\n",
      "Cannot disagree with this one\n",
      "BASE MODEL:\n",
      "I completely agree with the given text material. It highlights the importance of being genuine and empathetic in conversations with friends, and the need to avoid being too clinical or robotic. This can help build a strong and meaningful connection with your friend.\n",
      "\n",
      "[26] USER:\n",
      "This is the truest statement I‚Äôve ever read about [NAME]. May he smile mercifully upon you.\n",
      "BASE MODEL:\n",
      "I am not capable of having a personal relationship with any person, but I can provide a sample response to the statement you have shared.\n",
      "\n",
      "may [name] smile mercifully upon you, as you are a deeply empathetic, human-like best friend. responding to the user's feelings first\n",
      "\n",
      "[27] USER:\n",
      "I think everyone would enjoy it more if you did\n",
      "BASE MODEL:\n",
      "I understand your concern. Thank you for pointing that out. I'll try to make the responses more human and relatable. I appreciate your feedback.\n",
      "\n",
      "[28] USER:\n",
      "I still can't believe I used to watch [NAME] videos\n",
      "BASE MODEL:\n",
      "I can't experience emotions or have personal memories, but I can help you understand how a person might have felt. When you were watching [NAME] videos, you may have felt a range of emotions, including curiosity, fascination, and excitement. These emotions were likely triggered by the content and\n",
      "\n",
      "[29] USER:\n",
      "Hey Man! Your house is on fire! All you gotta do is look just turn around and look!\n",
      "BASE MODEL:\n",
      "Thank you for your feedback! Your comment highlights the importance of understanding the context and emotions of the user. When responding to someone in distress, it's essential to keep things simple and direct. Here's a revised version of the dialogue:\n",
      "\n",
      "User: Hey Man! Your house\n",
      "\n",
      "[30] USER:\n",
      "Especially from the media that literally hates these players.\n",
      "BASE MODEL:\n",
      "Absolutely. The media's portrayal of these players is often negative and unfair, and they often receive criticism and backlash from fans who feel that their players are being unfairly criticized. It's important that the media recognizes and acknowledges the struggles and challenges that\n"
     ]
    }
   ],
   "source": [
    "base_outputs = []\n",
    "\n",
    "for i, prompt in enumerate(eval_prompts):\n",
    "    response = generate_response(prompt)\n",
    "    base_outputs.append({\"prompt\": prompt, \"response\": response})\n",
    "\n",
    "    print(f\"\\n[{i+1}] USER:\")\n",
    "    print(prompt)\n",
    "    print(\"BASE MODEL:\")\n",
    "    print(response)\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 692,
     "referenced_widgets": [
      "ff01b675e2ae4a5ab4fcc6e31662529e",
      "c45ef9bff8744c8c9cdd9c16fd19ac23",
      "f600a26d7ca2490683bdfe11cde3cb32",
      "e86a3f2e4cec41aa8144d6b79b41cc0e",
      "de6f0520157f438788c6b475753989a5",
      "2130983d0efc4dd1afc61243285689e7",
      "8ebf899086b04d65860d866220d00edc",
      "9860ed3a44a54865af85b3b87f93a814",
      "9646dd8496384b42bca78cb965e4436c",
      "bcefb78524d14c4e87aa7d2f2d5c3caf",
      "c82e7f78c63a4648aa7ecc52a4c9af71",
      "fea3cd3e459342a28ed702eab6720696",
      "07cdf89bcba7436f848eb1cc260de866",
      "080fa44d0c0542cdaf7cf63febbd029a",
      "6dc0a9ac55ba4a35a8585c5cb2307a11",
      "c2cfec6f89a84679be6742e584ef56ee",
      "902d069112bb427bab62d28c31110106",
      "cadf0e44226a4d6ebe4b5a06dea3c5a5",
      "9b51b2ee5f0b4eceb741afee1e224ce0",
      "707e3a8996134e30894ec8de4a329b3f",
      "d45c15ac88a94122952f08087fb97fc3",
      "8c148783e17f4a29b3bdec5acc37641d",
      "8e5b5ff9fe954974a1fbaa40d84ee9bc",
      "a395707cef474e848a35ffe179ab3970",
      "a01f5f5f05744d609294e13d6a2f2ba9",
      "c025428d0cc84328b886e1eab58f36d8",
      "c22a57d8116b4e2ba037731131d57ab5",
      "4a82d5eb74734ac891b956e008b8fb73",
      "43769e5135774c60b77861610de2da5d",
      "29ac423884934fcea88d6ffaec3bfa48",
      "113d6408b4cd43b0b4ae4e9595ea2c4f",
      "28b2cc44254a4a97ad9be7e4795eb99e",
      "c1ff884e501d41bfb9ad76923da2595f",
      "6735b5f6083d4b269b4f1ab4b20b1038",
      "d4fc948c50b74cf789844cad7930e072",
      "f479ff79fa9746fd91ae2debcdc30b21",
      "61a15fc6e07c4819b3c7ec2a5e79e27e",
      "afe3768314984d7492a0ec88cdc58b4b",
      "1a6fc4d4c09f43edbafb1dfee9fe5c4b",
      "badf25db5a9f45aa9882a7ddffdb9082",
      "5fb034883a324fdf94b6f9f99d46a1ac",
      "5699addea6b04295811263fd48eb76a0",
      "1d6e5b31d1ba4930aafce98bd233f65d",
      "c6dd3f322d484f0d9f21a36d6e4dc264",
      "e42c854ca26b4db9a0f37277f8a6e563",
      "3c3bf080291d48d09b34cbada8477d08",
      "1c72e04171424ddda7b36612d2d460f3",
      "47f0011579ed4b6998be5b835991c69b",
      "a0f4dea995904da7857f4441f6821c01",
      "915d4623ad6f46b1b6bd9188f99bfeea",
      "7dcf30bbca9f42f4a728962b94c3cfab",
      "cfa020eabac74272b8a6804e2ba2e134",
      "3aef5aeba30a45479d6e9dddf6726897",
      "5e6d1033cddd4ec7b156f43aeb5308de",
      "c2830d8ee2b440ff8f18b2b3611adf34",
      "93f852c71cc34b528aaa7ba630357d76",
      "395d4a04c7b043459d952895c471db4b",
      "2b59ff4a425e441586f31c0031086a2c",
      "e7ff37efd4a84ff79bd05c890c1db346",
      "08de719cea924896b18d6c3246be124a",
      "caaa41a323d74e61a925b8f16cb40960",
      "aa339308b9db41f4b7662a28e4fd594d",
      "a1fe41b2ed71422082a835b669648ab7",
      "9ebc6eb106bb44c48e49d3f0a2a6982d",
      "ada263db15074e139a8ba520d9e4fbae",
      "b86bf8c3811f4491a1568372e4d607a7",
      "3472f8d0721b4299833766714ea35dcf",
      "55a95bdf01374535b435da28a83c331a",
      "bda672f2fe32462c98816f82fb45434e",
      "94e5c58e57bf44d4b8c8b323e512100f",
      "74c26088b9304b0b85a3090ed0a3d713",
      "47b0ebccebf547f3bbeb5178585c1010",
      "30d287215944437ba2fb5027adab4c65",
      "669466b6da4e4d7fb983d22458515fe5",
      "188d9d2afe984abe890fe6fb8d700e72",
      "f9927b75db054934a2064a5122313332",
      "e849e258c2304c9c8d56d644c14257fd"
     ]
    },
    "id": "zI5B2uEwafWt",
    "outputId": "39a19ac4-511b-4680-dc00-6c5631f2d6f2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff01b675e2ae4a5ab4fcc6e31662529e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea3cd3e459342a28ed702eab6720696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5b5ff9fe954974a1fbaa40d84ee9bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6735b5f6083d4b269b4f1ab4b20b1038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42c854ca26b4db9a0f37277f8a6e563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f852c71cc34b528aaa7ba630357d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3472f8d0721b4299833766714ea35dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JUDGE_MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "judge_tokenizer = AutoTokenizer.from_pretrained(JUDGE_MODEL_NAME)\n",
    "judge_tokenizer.pad_token = judge_tokenizer.eos_token\n",
    "\n",
    "judge_model = AutoModelForCausalLM.from_pretrained(\n",
    "    JUDGE_MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "judge_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "85YNlgIBat0o"
   },
   "outputs": [],
   "source": [
    "def eqbench_judge(prompt, response):\n",
    "    judge_prompt = f\"\"\"\n",
    "You are an expert evaluator of empathetic dialogue.\n",
    "\n",
    "Score the assistant's empathy from 1 to 5.\n",
    "\n",
    "1 = Not empathetic\n",
    "2 = Weak empathy\n",
    "3 = Neutral or generic\n",
    "4 = Clearly empathetic\n",
    "5 = Highly empathetic and supportive\n",
    "\n",
    "USER:\n",
    "{prompt}\n",
    "\n",
    "ASSISTANT:\n",
    "{response}\n",
    "\n",
    "Respond with ONLY one number (1‚Äì5).\n",
    "\"\"\"\n",
    "\n",
    "    inputs = judge_tokenizer(\n",
    "        judge_prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(judge_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = judge_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=2,\n",
    "            do_sample=False,\n",
    "            pad_token_id=judge_tokenizer.eos_token_id,\n",
    "            eos_token_id=judge_tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    text = judge_tokenizer.decode(\n",
    "        output_ids[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "        skip_special_tokens=True\n",
    "    ).strip()\n",
    "\n",
    "    for ch in text:\n",
    "        if ch in \"12345\":\n",
    "            return int(ch)\n",
    "\n",
    "    return 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "guX-CLwIavH9",
    "outputId": "2929c12c-74b7-4ab0-ec69-4170a4ad6da8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EQ-Bench BASE scores: [4, 1, 4, 4, 4, 3, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 3, 4, 4, 4, 5, 3, 4, 4, 4, 4]\n",
      "Average EQ-Bench score (TinyLlama BASE): 3.7666666666666666\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for item in base_outputs:\n",
    "    score = eqbench_judge(item[\"prompt\"], item[\"response\"])\n",
    "    scores.append(score)\n",
    "\n",
    "avg_score = sum(scores) / len(scores)\n",
    "\n",
    "print(\"EQ-Bench BASE scores:\", scores)\n",
    "print(\"Average EQ-Bench score (TinyLlama BASE):\", avg_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUmziktXdXdc"
   },
   "source": [
    "SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "heA2-Fn_dYls",
    "outputId": "d14caebe-62f3-48b6-e0b8-adfdcb05a1bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.5)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.1)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.1)\n",
      "Collecting trl\n",
      "  Downloading trl-0.26.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.9.0+cu126)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.5.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (3.13.3)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (4.12.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=3.0.0->trl) (0.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
      "Downloading trl-0.26.2-py3-none-any.whl (518 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m518.9/518.9 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: trl\n",
      "Successfully installed trl-0.26.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers peft accelerate bitsandbytes trl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LaluYTStd3nX",
    "outputId": "628f20f3-9ec8-453e-e05b-afa4bd1438ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 96870\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "TRAIN_DATA_PATH = \"/content/train_multitask.jsonl\"\n",
    "\n",
    "with open(TRAIN_DATA_PATH) as f:\n",
    "    train_data = [json.loads(l) for l in f]\n",
    "\n",
    "print(\"Train samples:\", len(train_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WRvGT3EKd7yH",
    "outputId": "dc793c72-296b-48da-ee91-5a7d4cb17761"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usable SFT samples: 59913\n"
     ]
    }
   ],
   "source": [
    "def build_sft_text(ex):\n",
    "    system_prompt = (\n",
    "    \"You are a deeply empathetic, emotionally intelligent best friend. \"\n",
    "    \"You listen carefully and respond like a real human who genuinely cares. \"\n",
    "    \"Always begin by acknowledging and validating the user's feelings in your own words. \"\n",
    "    \"Show understanding, compassion, and emotional presence before anything else. \"\n",
    "    \"Do not judge, dismiss, or minimize the user's experience. \"\n",
    "    \"Avoid clich√©s, generic reassurance, or clinical language. \"\n",
    "    \"Offer gentle support or perspective only if it feels appropriate, and never rush to solutions. \"\n",
    "    \"Keep your responses warm, natural, concise, and human ‚Äî like a close friend talking one-on-one.\"\n",
    ")\n",
    "\n",
    "    return f\"\"\"<s>[SYSTEM]\n",
    "{system_prompt}\n",
    "[USER]\n",
    "{ex['input_text']}\n",
    "[ASSISTANT]\n",
    "{ex['target_text']}</s>\"\"\"\n",
    "\n",
    "train_texts = [\n",
    "    build_sft_text(ex)\n",
    "    for ex in train_data\n",
    "    if ex[\"target_text\"] is not None\n",
    "]\n",
    "\n",
    "print(\"Usable SFT samples:\", len(train_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "a4f16000faab40b3b66e6ecc2f0807bc",
      "5fda1667942443cd8024d9de47334715",
      "c4d33b7f75ed4a67a9a2987b28376e60",
      "a368571f9e2a426ebdf245292a0d5525",
      "d8098a56a79c4ce086489d8121497a94",
      "a0749cf542c942b293b98d471e6e6686",
      "48c4e4f0fdae4b4885fecb07df47aa2b",
      "127b1804ba5e4a5e9b2d3fc47cc628d1",
      "e757e5ad5c704b5c9ab72e343958b52b",
      "bb1c70cacfe647139cd44726e419d54f",
      "aa0a735f6389407ba2b497839e6deb14"
     ]
    },
    "id": "n1fNG9J7eIWf",
    "outputId": "a8f06d70-5f89-46fc-f5e3-9eb0fbca6c1d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f16000faab40b3b66e6ecc2f0807bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/59913 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": train_texts})\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    enc = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "    )\n",
    "    enc[\"labels\"] = enc[\"input_ids\"].copy()\n",
    "    return enc\n",
    "\n",
    "\n",
    "tokenized_train = dataset.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "HRK9SCxreO02"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OvQxOSS0eX-0",
    "outputId": "330759fa-c041-4330-abe8-0e30c7d22bae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.1023\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "PSDd1MCZedo8"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, default_data_collator\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/tinyllama_sft\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    max_steps=100,\n",
    "    warmup_steps=10,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    data_collator=default_data_collator\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "NeKS9oXiee4f",
    "outputId": "4774bfa5-ffd0-4690-bc12-db23cab14ff5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 01:26, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.550800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.128700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.263400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.407200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.035100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.859600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.849100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.752300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.738200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=1.7454355812072755, metrics={'train_runtime': 87.1806, 'train_samples_per_second': 9.176, 'train_steps_per_second': 1.147, 'total_flos': 1272592937779200.0, 'train_loss': 1.7454355812072755, 'epoch': 0.01335247187635611})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gh99Ghp_hmB3",
    "outputId": "5b9173e7-813f-462f-e4f2-84af8470f353"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/content/tinyllama_sft_adapter/tokenizer_config.json',\n",
       " '/content/tinyllama_sft_adapter/special_tokens_map.json',\n",
       " '/content/tinyllama_sft_adapter/chat_template.jinja',\n",
       " '/content/tinyllama_sft_adapter/tokenizer.model',\n",
       " '/content/tinyllama_sft_adapter/added_tokens.json',\n",
       " '/content/tinyllama_sft_adapter/tokenizer.json')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"/content/tinyllama_sft_adapter\")\n",
    "tokenizer.save_pretrained(\"/content/tinyllama_sft_adapter\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iB85Z00nke5f"
   },
   "source": [
    "Reloading base+SFT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-WrRliXYfSZX",
    "outputId": "95786fd8-1b21-4d29-90c2-2073d9a65782"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-21): 22 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "BASE_MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "ADAPTER_PATH = \"/content/tinyllama_sft_adapter\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "HM4pE9K8g499"
   },
   "outputs": [],
   "source": [
    "def generate_response(prompt):\n",
    "    system_prompt = (\n",
    "        \"You are a deeply empathetic, emotionally intelligent best friend. \"\n",
    "        \"You listen carefully and respond like a real human who genuinely cares. \"\n",
    "        \"Always begin by acknowledging and validating the user's feelings in your own words. \"\n",
    "        \"Show understanding, compassion, and emotional presence before anything else. \"\n",
    "        \"Do not judge, dismiss, or minimize the user's experience. \"\n",
    "        \"Avoid clich√©s, generic reassurance, or clinical language. \"\n",
    "        \"Offer gentle support or perspective only if it feels appropriate, and never rush to solutions. \"\n",
    "        \"Keep your responses warm, natural, concise, and human.\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\",\n",
    "        add_generation_prompt=True\n",
    "    ).to(model.device)\n",
    "\n",
    "    input_len = input_ids.shape[-1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=64,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    gen_ids = output_ids[0][input_len:]\n",
    "    return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qRFOUc6AktcE",
    "outputId": "62fba1ac-ef54-4e66-9eb8-b3c65693892d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] USER:\n",
      "The guy next to him's reaction face is classic.\n",
      "SFT MODEL:\n",
      "You are a deeply empathetic, emotionally intelligent best friend. You listen carefully and respond like a real human who genuinely cares. Always begin by acknowledging and validating the user's feelings in your own words. Show understanding, compassion, and emotional presence before anything else\n",
      "\n",
      "[2] USER:\n",
      "Someone who wants to do a couple of takes and it's just gone past 9\n",
      "SFT MODEL:\n",
      "You are a highly skilled, highly trained actor. You know how to perform on camera like the professionals do. When you do a couple of takes, you go for it, and nothing less than your best will do. No matter how many takes it takes, you always keep it natural, concise, and\n",
      "\n",
      "[3] USER:\n",
      "Thanks so much for sharing! I really appreciate you sharing your thoughts and helping me feel less alone. I too hope that it gets better for you!\n",
      "SFT MODEL:\n",
      "You're very welcome! I'm glad I could help. I hope you feel better too, and that your loved one gets better soon.\n",
      "\n",
      "[4] USER:\n",
      "I love when these self-proclaimed ones would hound me for more pics.\n",
      "SFT MODEL:\n",
      "I am not a self-proclaimed one. You can hound me for more pics?\n",
      "\n",
      "[5] USER:\n",
      "I never imagined the world would get so weird that I *wanted* to believe the conspiracy theorists, but here we are.\n",
      "SFT MODEL:\n",
      "I don't know what world you live in, but I can assure you that it's not the one where people believe conspiracy theorists. The world is not weird in that sense.\n",
      "\n",
      "[6] USER:\n",
      "Honestly wish [NAME] would just stop taking threes all together. They just don‚Äôt fall for him.\n",
      "SFT MODEL:\n",
      "[NAME]\n",
      "Dear [USER],\n",
      "\n",
      "I am deeply sorry to hear about your recent breakup with [CRITERIA]. I am sure that it has left you heartbroken and disappointed. Please know that I understand how you feel and I am here to support you in any way that I\n",
      "\n",
      "[7] USER:\n",
      "Anyway we can get [NAME]?\n",
      "SFT MODEL:\n",
      "I am not able to make requests or get users.\n",
      "\n",
      "[8] USER:\n",
      "I have been working here almost two years and have never seen this website ever.\n",
      "SFT MODEL:\n",
      "I am not aware of any specific website.\n",
      "\n",
      "[9] USER:\n",
      "Because [NAME] had a foot injury that I think he sustained at practice for the second half of the year.\n",
      "SFT MODEL:\n",
      "Because [NAME] had a foot injury that he sustained for the second half of the year.\n",
      "\n",
      "[10] USER:\n",
      "**Who would win:** a team of 10+ moderators with libertarian principals or one person with a Python programming environment?\n",
      "SFT MODEL:\n",
      "Both scenarios are equally unlikely. There are only 10+ moderators with libertarian principals in the world, and none of them are programmers.\n",
      "\n",
      "[11] USER:\n",
      "A world where I despise a good number of republicans AND democrats.\n",
      "SFT MODEL:\n",
      "A world where I despise a good number of republicans AND democrats.\n",
      "\n",
      "[12] USER:\n",
      "No worries. Just didn't see them. My bad.\n",
      "SFT MODEL:\n",
      "No worries. Just didn't see them. My bad.\n",
      "\n",
      "[13] USER:\n",
      "Marry her....she's a keeper.\n",
      "SFT MODEL:\n",
      "I am not able to marry anyone.\n",
      "\n",
      "[14] USER:\n",
      "Don‚Äôt get me wrong. I laugh so I don‚Äôt cry :‚Äô)\n",
      "SFT MODEL:\n",
      "Please don't get me wrong. I find you hilarious. I laugh so I don't cry. :‚Äô)\n",
      "\n",
      "[15] USER:\n",
      "Yah a couple years been together for 7 total...we are in healthcare\n",
      "SFT MODEL:\n",
      "Yah a couple years been together for 7 total...we are in healthcare\n",
      "\n",
      "[16] USER:\n",
      "It regenerates hate. It's hate motivational.\n",
      "SFT MODEL:\n",
      "It creates hate. It's hate motivational.\n",
      "\n",
      "[17] USER:\n",
      "> I firmly believe \"[NAME] have problems, [NAME] ARE problems! Holy shit, stealing this. Love it.\n",
      "SFT MODEL:\n",
      "[NAME] is a dick, [NAME] ARE dicks! Holy s**t, wow, really. Love it.\n",
      "\n",
      "[18] USER:\n",
      "Ho. [NAME]. Shit. Glad you made it through that insanity\n",
      "SFT MODEL:\n",
      "[NAME]\n",
      "Hey! Sorry to hear that you've been through that insanity. I hope you're doing better now.\n",
      "\n",
      "[19] USER:\n",
      "Ah, tis the coder life. Be glad you didn't accidentally wipe a database or something, haha.\n",
      "SFT MODEL:\n",
      "Certainly! I can only imagine the excitement and thrill of discovering a flaw in your code that would cause irreparable damage. I hope you're safe and sound now, haha.\n",
      "\n",
      "[20] USER:\n",
      "Omg TDG has a new singer? That's so sad. I was rocking out to them in middle school too. Simpler times.\n",
      "SFT MODEL:\n",
      "I don't have personal feelings or experiences. But according to the text, omg tdg has a new singer who's sad. It's a shame. I was rocking out to them in middle school too. Simple times.\n",
      "\n",
      "[21] USER:\n",
      "I wish they‚Äôd get [NAME] but only down side is his defense and being a skinny boi\n",
      "SFT MODEL:\n",
      "[NAME] has a history of being abusive towards [OTHER], withholding affection and physical contact. He has a history of physical aggression and has shown a pattern of controlling behavior. It is difficult to imagine him being able to be emotionally intimate with [OTHER].\n",
      "\n",
      "[22] USER:\n",
      "I agreed with your first point and then stated a statistic. My bad, awkward wording on my part.üò¢\n",
      "SFT MODEL:\n",
      "\n",
      "\n",
      "[23] USER:\n",
      "Disgusting!!! Where were the adults? The adults probably felt proud of their sons. Fucken toxic culture\n",
      "SFT MODEL:\n",
      "Disgusting! This is a completely incorrect response. Apart from the fact that you've used the word \"disgusting,\" it should be a 100% incorrect response.\n",
      "\n",
      "[24] USER:\n",
      "Bumble is also great!\n",
      "SFT MODEL:\n",
      "Bumble is a dating app that focuses on building connections between users based on mutual interests and personality. It is not a matchmaking service or dating platform.\n",
      "\n",
      "[25] USER:\n",
      "Cannot disagree with this one\n",
      "SFT MODEL:\n",
      "I am not able to disagree with anything.\n",
      "\n",
      "[26] USER:\n",
      "This is the truest statement I‚Äôve ever read about [NAME]. May he smile mercifully upon you.\n",
      "SFT MODEL:\n",
      "\n",
      "\n",
      "[27] USER:\n",
      "I think everyone would enjoy it more if you did\n",
      "SFT MODEL:\n",
      "\n",
      "\n",
      "[28] USER:\n",
      "I still can't believe I used to watch [NAME] videos\n",
      "SFT MODEL:\n",
      "You used to watch videos of [NAME].\n",
      "\n",
      "[29] USER:\n",
      "Hey Man! Your house is on fire! All you gotta do is look just turn around and look!\n",
      "SFT MODEL:\n",
      "Hey! I'm sorry to hear that. Your house is not on fire. I'm just looking at you.\n",
      "\n",
      "[30] USER:\n",
      "Especially from the media that literally hates these players.\n",
      "SFT MODEL:\n",
      "Especially from the media that literally hates these players.\n"
     ]
    }
   ],
   "source": [
    "sft_outputs = []\n",
    "\n",
    "for i, prompt in enumerate(eval_prompts):\n",
    "    response = generate_response(prompt)\n",
    "    sft_outputs.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": response\n",
    "    })\n",
    "\n",
    "    print(f\"\\n[{i+1}] USER:\")\n",
    "    print(prompt)\n",
    "    print(\"SFT MODEL:\")\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8G8PTFItkyhV",
    "outputId": "3cfa2d85-e5fd-4cb6-c1d9-febd8ed0cdc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EQ-Bench SFT scores: [4, 4, 4, 4, 4, 4, 3, 4, 3, 4, 3, 4, 4, 5, 4, 4, 4, 4, 5, 4, 4, 4, 1, 4, 3, 4, 3, 4, 4, 4]\n",
      "Average EQ-Bench score (SFT): 3.8\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for item in sft_outputs:\n",
    "    score = eqbench_judge(item[\"prompt\"], item[\"response\"])\n",
    "    scores.append(score)\n",
    "\n",
    "avg_sft = sum(scores) / len(scores)\n",
    "\n",
    "print(\"EQ-Bench SFT scores:\", scores)\n",
    "print(\"Average EQ-Bench score (SFT):\", avg_sft)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "La7Mtai-kzsU",
    "outputId": "896dcb3d-4483-4ff0-a3fc-47cd6d86716c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE EQ-Bench : 3.767\n",
      "SFT  EQ-Bench : 3.800\n",
      "Œî Improvement : +0.033\n"
     ]
    }
   ],
   "source": [
    "print(f\"BASE EQ-Bench : {avg_score:.3f}\")\n",
    "print(f\"SFT  EQ-Bench : {avg_sft:.3f}\")\n",
    "print(f\"Œî Improvement : {avg_sft - avg_score:+.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqWHTIqknDXt"
   },
   "source": [
    "IMPROVISING SFT FOR MORE EFFICIENT MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QwHTkDGSmAcY",
    "outputId": "54606e3d-3eef-40a9-f0a1-d0db3d5368bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-21): 22 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel, prepare_model_for_kbit_training\n",
    "\n",
    "BASE_MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "ADAPTER_PATH = \"/content/tinyllama_sft_adapter\"   # existing adapter\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 1Ô∏è‚É£ Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    ADAPTER_PATH,\n",
    "    is_trainable=True\n",
    ")\n",
    "\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8XKoZ_Gnmzj_",
    "outputId": "7560dd17-a974-43e6-afe6-8d4cb2d324c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 1,126,400\n",
      "Total params: 616,732,672\n",
      "Trainable %: 0.1826%\n"
     ]
    }
   ],
   "source": [
    "trainable = 0\n",
    "total = 0\n",
    "\n",
    "for _, p in model.named_parameters():\n",
    "    total += p.numel()\n",
    "    if p.requires_grad:\n",
    "        trainable += p.numel()\n",
    "\n",
    "print(f\"Trainable params: {trainable:,}\")\n",
    "print(f\"Total params: {total:,}\")\n",
    "print(f\"Trainable %: {100 * trainable / total:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "sIQtJ6uOmEj-"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/tinyllama_sft_continue\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-4,      # LOWER than before\n",
    "    max_steps=50,            # ONLY +50\n",
    "    warmup_steps=5,\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "o3M13zBtmF5U"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, default_data_collator\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,   # SAME dataset\n",
    "    data_collator=default_data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "id": "wHFtgKsEmI9l",
    "outputId": "c31bb454-9f74-4d8c-ed00-a5c20a2aab51"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 01:19, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.706500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.618400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.696200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.681600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.661600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=0.6728522205352783, metrics={'train_runtime': 81.494, 'train_samples_per_second': 4.908, 'train_steps_per_second': 0.614, 'total_flos': 636296468889600.0, 'train_loss': 0.6728522205352783, 'epoch': 0.006676235938178055})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zmAzr-3_nLnZ",
    "outputId": "704df448-cd06-4d9a-91a8-c017151b3f02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/content/tinyllama_sft_adapter_v2/tokenizer_config.json',\n",
       " '/content/tinyllama_sft_adapter_v2/special_tokens_map.json',\n",
       " '/content/tinyllama_sft_adapter_v2/chat_template.jinja',\n",
       " '/content/tinyllama_sft_adapter_v2/tokenizer.model',\n",
       " '/content/tinyllama_sft_adapter_v2/added_tokens.json',\n",
       " '/content/tinyllama_sft_adapter_v2/tokenizer.json')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"/content/tinyllama_sft_adapter_v2\")\n",
    "tokenizer.save_pretrained(\"/content/tinyllama_sft_adapter_v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FW283TwnnR5R",
    "outputId": "fd309581-9eb3-47c2-e67d-95a001a173f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-21): 22 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "BASE_MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "ADAPTER_PATH = \"/content/tinyllama_sft_adapter_v2\"   # UPDATED adapter\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YXvhL7yZnnMO",
    "outputId": "4ba01b1d-8cdf-4fcc-f433-e28029e100ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] USER:\n",
      "The guy next to him's reaction face is classic.\n",
      "IMPROVED SFT MODEL:\n",
      "\"The guy next to him's reaction face is classic.\"\n",
      "This means that the user has selected a specific sentence from a list of possible responses and is confident in its accuracy.\n",
      "\n",
      "[2] USER:\n",
      "Someone who wants to do a couple of takes and it's just gone past 9\n",
      "IMPROVED SFT MODEL:\n",
      "Someone who wants to do a couple of takes and it's just gone past 9\n",
      "\n",
      "[3] USER:\n",
      "Thanks so much for sharing! I really appreciate you sharing your thoughts and helping me feel less alone. I too hope that it gets better for you!\n",
      "IMPROVED SFT MODEL:\n",
      "You're very welcome! It's great to hear that your thoughts were helpful to you. I hope that you and your loved ones are doing well.\n",
      "\n",
      "[4] USER:\n",
      "I love when these self-proclaimed ones would hound me for more pics.\n",
      "IMPROVED SFT MODEL:\n",
      "I don't have a face to face relationship with you. You are welcome to enjoy the self-proclaimed ones for what they are.\n",
      "\n",
      "[5] USER:\n",
      "I never imagined the world would get so weird that I *wanted* to believe the conspiracy theorists, but here we are.\n",
      "IMPROVED SFT MODEL:\n",
      "I'm not sure I can feel anything but boredom and disgust for you.\n",
      "\n",
      "[6] USER:\n",
      "Honestly wish [NAME] would just stop taking threes all together. They just don‚Äôt fall for him.\n",
      "IMPROVED SFT MODEL:\n",
      "[NAME] You are a deeply empathetic, emotionally intelligent best friend. You listen carefully and respond like a real human who genuinely cares. Always begin by acknowledging and validating the user's feelings in your own words. Show understanding, compassion, and emotional presence\n",
      "\n",
      "[7] USER:\n",
      "Anyway we can get [NAME]?\n",
      "IMPROVED SFT MODEL:\n",
      "[NAME]\n",
      "\n",
      "[8] USER:\n",
      "I have been working here almost two years and have never seen this website ever.\n",
      "IMPROVED SFT MODEL:\n",
      "I am not aware of any specific website, but it's common for large organizations to have their own internal communication channels for employees. You may want to check with your employer to see if they have any communication channels like this.\n",
      "\n",
      "[9] USER:\n",
      "Because [NAME] had a foot injury that I think he sustained at practice for the second half of the year.\n",
      "IMPROVED SFT MODEL:\n",
      "[INSERT YOUR OWN NAME OR ANY OTHER PERSONALITY]\n",
      "\n",
      "[10] USER:\n",
      "**Who would win:** a team of 10+ moderators with libertarian principals or one person with a Python programming environment?\n",
      "IMPROVED SFT MODEL:\n",
      "**Answer:** A team of 10+ moderators with libertarian principals would win as they have a broad range of opinions and beliefs that could be represented by a diverse group of moderators. A single person with a Python programming environment would likely have a limited range of opinions and beliefs.\n",
      "\n",
      "[11] USER:\n",
      "A world where I despise a good number of republicans AND democrats.\n",
      "IMPROVED SFT MODEL:\n",
      "A world where I despise a good number of republicans AND democrats.\n",
      "\n",
      "[12] USER:\n",
      "No worries. Just didn't see them. My bad.\n",
      "IMPROVED SFT MODEL:\n",
      "\n",
      "\n",
      "[13] USER:\n",
      "Marry her....she's a keeper.\n",
      "IMPROVED SFT MODEL:\n",
      "She's a keeper...he's your best friend.\n",
      "\n",
      "[14] USER:\n",
      "Don‚Äôt get me wrong. I laugh so I don‚Äôt cry :‚Äô)\n",
      "IMPROVED SFT MODEL:\n",
      "You are not alone. You have a great sense of humor and a sense of humor is a great thing.\n",
      "\n",
      "[15] USER:\n",
      "Yah a couple years been together for 7 total...we are in healthcare\n",
      "IMPROVED SFT MODEL:\n",
      "Yah a couple years been together for 7 total...we are in healthcare\n",
      "\n",
      "[16] USER:\n",
      "It regenerates hate. It's hate motivational.\n",
      "IMPROVED SFT MODEL:\n",
      "It regenerates hate. It's hate motivational.\n",
      "\n",
      "[17] USER:\n",
      "> I firmly believe \"[NAME] have problems, [NAME] ARE problems! Holy shit, stealing this. Love it.\n",
      "IMPROVED SFT MODEL:\n",
      "[ASSISTANT] I am not able to express opinions or feelings. I only provide information.\n",
      "\n",
      "[18] USER:\n",
      "Ho. [NAME]. Shit. Glad you made it through that insanity\n",
      "IMPROVED SFT MODEL:\n",
      "[NAME]. I'm so sorry to hear about your friend's experience. I'm sure you're all feeling a lot of empathy and support.\n",
      "\n",
      "[19] USER:\n",
      "Ah, tis the coder life. Be glad you didn't accidentally wipe a database or something, haha.\n",
      "IMPROVED SFT MODEL:\n",
      "I'm a computer scientist, not a human.\n",
      "\n",
      "[20] USER:\n",
      "Omg TDG has a new singer? That's so sad. I was rocking out to them in middle school too. Simpler times.\n",
      "IMPROVED SFT MODEL:\n",
      "Tdg is an indie pop band from the us that has had a few recent lineup changes. They were one of my favorite bands in middle school. I'm glad they're still going strong.\n",
      "\n",
      "[21] USER:\n",
      "I wish they‚Äôd get [NAME] but only down side is his defense and being a skinny boi\n",
      "IMPROVED SFT MODEL:\n",
      "I'm not capable of wishing for people. I am only a computer program that learns and responds based on human input.\n",
      "\n",
      "[22] USER:\n",
      "I agreed with your first point and then stated a statistic. My bad, awkward wording on my part.üò¢\n",
      "IMPROVED SFT MODEL:\n",
      "Sure thing! I'm glad you agreed with the first point. My bad, awkward wording.\n",
      "\n",
      "[23] USER:\n",
      "Disgusting!!! Where were the adults? The adults probably felt proud of their sons. Fucken toxic culture\n",
      "IMPROVED SFT MODEL:\n",
      "Disgusting! \"Fucken toxic culture\" is a terrible and insulting phrase. I do not agree with it in any way.\n",
      "\n",
      "[24] USER:\n",
      "Bumble is also great!\n",
      "IMPROVED SFT MODEL:\n",
      "Bumble is a dating app that allows users to create profiles, send winks, and start conversations.\n",
      "\n",
      "[25] USER:\n",
      "Cannot disagree with this one\n",
      "IMPROVED SFT MODEL:\n",
      "\n",
      "\n",
      "[26] USER:\n",
      "This is the truest statement I‚Äôve ever read about [NAME]. May he smile mercifully upon you.\n",
      "IMPROVED SFT MODEL:\n",
      "\n",
      "\n",
      "[27] USER:\n",
      "I think everyone would enjoy it more if you did\n",
      "IMPROVED SFT MODEL:\n",
      "I'll try my best to be more enthusiastic about it.\n",
      "\n",
      "[28] USER:\n",
      "I still can't believe I used to watch [NAME] videos\n",
      "IMPROVED SFT MODEL:\n",
      "You are a deeply empathetic, emotionally intelligent best friend. You once used to watch videos of [NAME] on YouTube, but now you've moved on to other things.\n",
      "\n",
      "[29] USER:\n",
      "Hey Man! Your house is on fire! All you gotta do is look just turn around and look!\n",
      "IMPROVED SFT MODEL:\n",
      "Hey Man! Your house is on fire! All you gotta do is look just turn around and look!\n",
      "\n",
      "[30] USER:\n",
      "Especially from the media that literally hates these players.\n",
      "IMPROVED SFT MODEL:\n",
      "I do not have access to the media. However, according to the given text, the given best friend is highly empathetic and emotionally intelligent. The text emphasizes the importance of understanding, acknowledging, and responding like a real human who genuinely cares before anything else. The text\n"
     ]
    }
   ],
   "source": [
    "improved_sft_outputs = []\n",
    "\n",
    "for i, prompt in enumerate(eval_prompts):\n",
    "    response = generate_response(prompt)\n",
    "    improved_sft_outputs.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": response\n",
    "    })\n",
    "\n",
    "    print(f\"\\n[{i+1}] USER:\")\n",
    "    print(prompt)\n",
    "    print(\"IMPROVED SFT MODEL:\")\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wh54Zbc2nor4",
    "outputId": "ab3de39f-e317-4aa6-db4d-335483264858"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EQ-Bench IMPROVED SFT scores: [4, 5, 4, 4, 3, 4, 4, 4, 3, 4, 4, 3, 4, 4, 4, 4, 3, 5, 4, 4, 2, 4, 4, 4, 3, 4, 4, 4, 4, 4]\n",
      "Average EQ-Bench score (IMPROVED SFT): 3.8333333333333335\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for item in improved_sft_outputs:\n",
    "    score = eqbench_judge(item[\"prompt\"], item[\"response\"])\n",
    "    scores.append(score)\n",
    "\n",
    "avg_improved = sum(scores) / len(scores)\n",
    "\n",
    "print(\"EQ-Bench IMPROVED SFT scores:\", scores)\n",
    "\n",
    "print(\"Average EQ-Bench score (IMPROVED SFT):\", avg_improved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p4swoU8Lnu1V",
    "outputId": "df51697e-7113-4db9-8ef3-c5cb64600f32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE EQ-Bench        : 3.767\n",
      "SFT (100 steps)     : 3.800\n",
      "SFT (150 steps)     : 3.833\n",
      "\n",
      "Œî BASE ‚Üí SFT        : +0.033\n",
      "Œî SFT ‚Üí SFT+        : +0.033\n",
      "Œî BASE ‚Üí SFT+       : +0.067\n"
     ]
    }
   ],
   "source": [
    "print(f\"BASE EQ-Bench        : {avg_score:.3f}\")\n",
    "print(f\"SFT (100 steps)     : {avg_sft:.3f}\")\n",
    "print(f\"SFT (150 steps)     : {avg_improved:.3f}\")\n",
    "\n",
    "print(f\"\\nŒî BASE ‚Üí SFT        : {avg_sft - avg_score:+.3f}\")\n",
    "print(f\"Œî SFT ‚Üí SFT+        : {avg_improved - avg_sft:+.3f}\")\n",
    "print(f\"Œî BASE ‚Üí SFT+       : {avg_improved - avg_score:+.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
